{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "In this assignment, your task is to train a ML model for text classification task.\n",
    "\n",
    "Allowed libraries are basic python libraries, numpy, pandas, re, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "## Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Download the 'tweeteval' emotion dataset from https://huggingface.co/datasets/tweet_eval/tree/main/dummy/emotion/1.1.0 and read the training corpus with $pandas$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Worry is a down payment on a problem you may ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My roommate: it's okay that we can't spell bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No but that's so cute. Atsu was probably shy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rooneys fucking untouchable isn't he? Been fuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it's pretty depressing when u hit pan on ur fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  “Worry is a down payment on a problem you may ...\n",
       "1  My roommate: it's okay that we can't spell bec...\n",
       "2  No but that's so cute. Atsu was probably shy a...\n",
       "3  Rooneys fucking untouchable isn't he? Been fuc...\n",
       "4  it's pretty depressing when u hit pan on ur fa..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code\n",
    "df = pd.read_csv('dummy_data/train_text.txt', sep='\\t', header=None)\n",
    "df.columns = ['text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Lowercase the text in the training courpus and then use Spacy to tokenize and lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\", worry, be, a, down, payment, on, a, proble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[my, roommate, :, it, be, okay, that, we, can,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[no, but, that, be, so, cute, ., atsu, be, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[rooney, fuck, untouchable, be, not, he, ?, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[it, be, pretty, depressing, when, u, hit, pan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [\", worry, be, a, down, payment, on, a, proble...\n",
       "1  [my, roommate, :, it, be, okay, that, we, can,...\n",
       "2  [no, but, that, be, so, cute, ., atsu, be, pro...\n",
       "3  [rooney, fuck, untouchable, be, not, he, ?, be...\n",
       "4  [it, be, pretty, depressing, when, u, hit, pan..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text'] = df['text'].apply(lambda x: [token.lemma_ for token in nlp(x)])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Build a vocabulary after removing (i) stop words, (ii) punctuation marks and symbols,  and (iii) the words with frequence 2 or less. To find the stop words, sort the words by their count in descending order and identify manually the words not related to \"emotion\". You can only use basic python and regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# remove punctuation marks and symbols using regex\n",
    "df['text'] = df['text'].apply(lambda x: [re.sub(r'[^\\w\\s]','',word) for word in x])\n",
    "\n",
    "# remove stop words\n",
    "stop = stopwords.words('english')\n",
    "df['text'] = df['text'].apply(lambda x: [token for token in x if token not in stop])\n",
    "\n",
    "# remove words with frequency 2 or less\n",
    "freq = pd.Series(' '.join(df['text'].astype(str)).split()).value_counts()\n",
    "freq = freq[freq <= 2]\n",
    "df['text'] = df['text'].apply(lambda x: [token for token in x if token not in freq])\n",
    "\n",
    "\n",
    "# remove empty strings\n",
    "df['text'] = df['text'].apply(lambda x: [token for token in x if token != ''])\n",
    "\n",
    "# sort the words by their count in descending order\n",
    "freq = pd.Series(' '.join(df['text'].astype(str)).split()).value_counts()\n",
    "freq = freq.sort_values(ascending=False)\n",
    "freq\n",
    "\n",
    "# build the vocabulary\n",
    "vocab = freq.index.tolist()\n",
    "vocab\n",
    "\n",
    "\n",
    "\n",
    "# save the vocabulary\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for item in vocab:\n",
    "        f.write(\"%s \" % item)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Consider words in the vocabulary as features and convert each sample/text to a bag or word represetnation. This will be your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('vocab.txt', 'r') as f:\n",
    "    vocab = f.read().split()\n",
    "\n",
    "# convert each sample/text to a bag or word represetnation\n",
    "def convert_to_bow(text):\n",
    "    bow = np.zeros(len(vocab))\n",
    "    for word in text:\n",
    "        if word in vocab:\n",
    "            bow[vocab.index(word)] += 1\n",
    "    return bow\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: convert_to_bow(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Repeat steps (a), (b), and (d) for test corpus to obtain the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'feel',\",\n",
       " \"['user',\",\n",
       " \"'mpsvt',\",\n",
       " \"'open',\",\n",
       " \"'depression']\",\n",
       " \"'welcome',\",\n",
       " \"'joyful']\",\n",
       " \"'20yrs',\",\n",
       " \"'still']\",\n",
       " \"'government',\",\n",
       " \"'trauma',\",\n",
       " \"'hospital',\",\n",
       " \"['deppression',\",\n",
       " \"'choice',\",\n",
       " \"'people',\",\n",
       " \"'image',\",\n",
       " \"'brother',\",\n",
       " \"'add',\",\n",
       " \"'grateful',\",\n",
       " \"'relationship']\",\n",
       " \"'bad']\",\n",
       " \"['make',\",\n",
       " \"'accident',\",\n",
       " \"'anxiety',\",\n",
       " \"'fund',\",\n",
       " \"'ampmake',\",\n",
       " \"'w',\",\n",
       " \"'depth',\",\n",
       " \"'care',\",\n",
       " \"'dead',\",\n",
       " \"['visit',\",\n",
       " \"'terrorism',\",\n",
       " \"'ago',\",\n",
       " \"'confirm',\",\n",
       " \"'affect',\",\n",
       " \"'trigger',\",\n",
       " \"'door',\",\n",
       " \"'delighted',\",\n",
       " \"'partner',\",\n",
       " \"'understand',\",\n",
       " \"'symptom',\",\n",
       " \"'truly',\",\n",
       " \"'real',\",\n",
       " \"'depress',\",\n",
       " \"'word',\",\n",
       " \"'interesting',\",\n",
       " \"'bit',\"]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_test = pd.read_csv('dummy_data/test_text.txt', sep='\\t', header=None)\n",
    "df_test.columns = ['text']\n",
    "\n",
    "# lowercase the text in the training courpus and then use Spacy to tokenize and lemmatize\n",
    "df_test['text'] = df_test['text'].str.lower()\n",
    "df_test['text'] = df_test['text'].apply(lambda x: [token.lemma_ for token in nlp(x)])\n",
    "\n",
    "# remove punctuation marks and symbols using regex\n",
    "df_test['text'] = df_test['text'].apply(lambda x: [re.sub(r'[^\\w\\s]','',word) for word in x])\n",
    "\n",
    "# remove stop words\n",
    "stop = stopwords.words('english')\n",
    "df_test['text'] = df_test['text'].apply(lambda x: [token for token in x if token not in stop])\n",
    "\n",
    "# remove words with frequency 2 or less\n",
    "freq = pd.Series(' '.join(df_test['text'].astype(str)).split()).value_counts()\n",
    "freq = freq[freq <= 2]\n",
    "df_test['text'] = df_test['text'].apply(lambda x: [token for token in x if token not in freq])\n",
    "\n",
    "# remove empty strings\n",
    "df_test['text'] = df_test['text'].apply(lambda x: [token for token in x if token != ''])\n",
    "\n",
    "# sort the words by their count in descending order\n",
    "freq = pd.Series(' '.join(df_test['text'].astype(str)).split()).value_counts()\n",
    "freq = freq.sort_values(ascending=False)\n",
    "\n",
    "# build the vocabulary\n",
    "vocab = freq.index.tolist()\n",
    "vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 \n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a support vector maching with RBF kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a) Do 3-fold cross-validation to pick the optimum $gamma$ hyperparameter in RBF kernel in $sklearn.svm.SVC$.code\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). Use \"micro-averaged\" f1-score for test score. \n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# read the training labels\n",
    "df_label = pd.read_csv('dummy_data/train_labels.txt', sep='\\t', header=None)\n",
    "df_label.columns = ['label']\n",
    "\n",
    "# combine the training data and labels\n",
    "df = pd.concat([df, df_label], axis=1)\n",
    "df.head()\n",
    "\n",
    "# split the training data into 3 folds for cross-validation using n_splits=3\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "kf.get_n_splits(df)\n",
    "\n",
    "# pick the optimum gamma hyperparameter in RBF kernel\n",
    "param_grid = {'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=kf, scoring='f1_micro')\n",
    "grid.fit(df['text'].tolist(), df['label'].tolist())\n",
    "\n",
    "# print the best parameters\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot test score vs gamma. Discuss the gamma values for which the model is underfit and overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'param_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# plot test score vs gamma\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m plt\u001b[39m.\u001b[39mplot(param_grid[\u001b[39m'\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m'\u001b[39m], grid\u001b[39m.\u001b[39mcv_results_[\u001b[39m'\u001b[39m\u001b[39mdummy_data/train_text.txt\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mtest score\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'param_grid' is not defined"
     ]
    }
   ],
   "source": [
    "# code\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# plot test score vs gamma\n",
    "plt.plot(param_grid['gamma'], grid.cv_results_['dummy_data/train_text.txt'])\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('test score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) use the best hyperparameter selected in (a) to  train on the entier dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma=0.001)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train on the entier dataset\n",
    "clf = SVC(kernel='rbf', gamma=0.001)\n",
    "clf.fit(df['text'].tolist(), df['label'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 \n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Find the confusion matrix and discuss your observations. Use *sklearn.metrics.confusion_matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Find the confusion matrix and discuss your observations. Use *sklearn.metrics.confusion_matrix*.\n",
    "\n",
    "# code\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# bag of words representation of the test data using the vocabulary from the training data\n",
    "df_test['text'] = df_test['text'].apply(lambda x: convert_to_bow(x))\n",
    "\n",
    "# fix length of the test data to match your training data\n",
    "for i in range(len(df_test)):\n",
    "    if len(df_test['text'][i]) < len(df['text'][0]):\n",
    "        df_test['text'][i] = np.append(df_test['text'][i], np.zeros(len(df['text'][0]) - len(df_test['text'][i])))\n",
    "    elif len(df_test['text'][i]) > len(df['text'][0]):\n",
    "        df_test['text'][i] = df_test['text'][i][:len(df['text'][0])]\n",
    "\n",
    "# predict the labels for the test data\n",
    "y_pred = clf.predict(df_test['text'].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Calculate micro-averaged and macro-averaged precision, recall, and F1 score. See the documentation for *sklearn.metrics.f1_score*, *sklearn.metrics.precision_score*, and *sklearn.metrics.recall_score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged precision: 0.40\n",
      "Micro-averaged recall: 0.40\n",
      "Micro-averaged F1 score: 0.40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate micro-averaged and macro-averaged precision, recall, and F1 score. See the documentation for *sklearn.metrics.f1_score*, *sklearn.metrics.precision_score*, and *sklearn.metrics.recall_score*.\n",
    "\n",
    "# code\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# print the micro-averaged precision, recall, and F1 score\n",
    "print('Micro-averaged precision: {:.2f}'.format(precision_score(df_label['label'].tolist(), y_pred, average='micro')))\n",
    "print('Micro-averaged recall: {:.2f}'.format(recall_score(df_label['label'].tolist(), y_pred, average='micro')))\n",
    "print('Micro-averaged F1 score: {:.2f}'.format(f1_score(df_label['label'].tolist(), y_pred, average='micro')))\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Calculate overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged precision: 0.10\n",
      "Macro-averaged recall: 0.25\n",
      "Macro-averaged F1 score: 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/retech/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# overal precision, recall, and F1 score\n",
    "print('Macro-averaged precision: {:.2f}'.format(precision_score(df_label['label'].tolist(), y_pred, average='macro')))\n",
    "print('Macro-averaged recall: {:.2f}'.format(recall_score(df_label['label'].tolist(), y_pred, average='macro')))\n",
    "print('Macro-averaged F1 score: {:.2f}'.format(f1_score(df_label['label'].tolist(), y_pred, average='macro')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "086a0dd6483ce3898d8007f6e7546959472f7137d666ff60ce6caa3018632df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
